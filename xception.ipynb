{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 자세한 내용은\n",
    "# 케라스 코리아 김태영 님의 자료 https://tykimos.github.io/2017/06/10/CNN_Data_Augmentation/\n",
    "# 를 추천합니다.\n",
    "\n",
    "params = {\n",
    "    # Generator Parameter\n",
    "    'random_state': 2,\n",
    "    # 회전하는 최대 각도\n",
    "    'rotation_range': 10,\n",
    "    # 좌우로 이동할 최대 비율\n",
    "    'width_shift_range': 0.20,\n",
    "    # 상하로 이동할 최대 비율\n",
    "    'height_shift_range': 0.20,\n",
    "    # 회전 및 밀림 값의 최대 라디안\n",
    "    'shear_range': 0.50,\n",
    "    # 축소/확대 할 최대 비율\n",
    "    'zoom_range': 0.20,\n",
    "    'horizontal_flip': True,\n",
    "    'brightness_range': (0.7, 1.5),\n",
    "    # Model Parameter\n",
    "    # xception 의 경우 (299, 299)를 많이 사용합니다.\n",
    "    'img_size': (299, 299),\n",
    "    'input_shape': (299, 299, 3),\n",
    "    'batch_size': 16,\n",
    "    # 한 번 split하여 generate 한 데이터 셋을 학습할 횟수\n",
    "    'epochs_per_generator': 5,\n",
    "    # Batch를 불러올 때 Multiprocessing을 사용합니다.\n",
    "    # 라이젠 등 코어가 많은 CPU 환경에서 많은 성능 향상을 가져올 수 있습니다.\n",
    "    'nb_workers': multiprocessing.cpu_count() // 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class.csv',\n",
       " 'cropped_test',\n",
       " 'cropped_train',\n",
       " 'inception_resnet_v2-0.86695',\n",
       " 'sample_submission.csv',\n",
       " 'submission.csv',\n",
       " 'test',\n",
       " 'test.csv',\n",
       " 'train',\n",
       " 'train.csv',\n",
       " 'xception_ep001_vloss-5.2130_vacc-0.0105.h5',\n",
       " 'xception_ep007_vloss-8.2031_vacc-0.0115.h5',\n",
       " 'xception_ep008_vloss-4.5204_vacc-0.0329.h5',\n",
       " 'xception_ep009_vloss-4.0916_vacc-0.0634.h5',\n",
       " 'xception_ep010_vloss-3.6161_vacc-0.1163.h5',\n",
       " 'xception_ep011_vloss-2.9675_vacc-0.2111.h5',\n",
       " 'xception_ep012_vloss-2.7274_vacc-0.3089.h5',\n",
       " 'xception_ep013_vloss-2.0129_vacc-0.4441.h5',\n",
       " 'xception_ep014_vloss-1.9834_vacc-0.4805.h5',\n",
       " 'xception_ep015_vloss-1.4812_vacc-0.5828.h5',\n",
       " 'xception_ep016_vloss-0.9499_vacc-0.7101.h5',\n",
       " 'xception_ep017_vloss-0.9059_vacc-0.7146.h5',\n",
       " 'xception_ep018_vloss-0.9484_vacc-0.7325.h5',\n",
       " 'xception_ep020_vloss-0.7427_vacc-0.7725.h5',\n",
       " 'xception_ep021_vloss-0.4973_vacc-0.8568.h5',\n",
       " 'xception_ep025_vloss-0.5598_vacc-0.8463.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 혹 다른 데이터 셋 추가(Pretrained Model Weights)로 인해 PATH가 변경된다면 아래 PATH를 수정\n",
    "DATA_PATH = 'E:\\\\data\\\\2019-3rd-ml-month-with-kakr'\n",
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 폴더 경로\n",
    "TRAIN_IMG_PATH = os.path.join(DATA_PATH, 'cropped_train')\n",
    "TEST_IMG_PATH = os.path.join(DATA_PATH, 'cropped_test')\n",
    "\n",
    "# CSV 파일 경로\n",
    "df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "df_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class를 정수 값에서 문자열로 치환합니다.\n",
    "df_train['class'] = df_train['class'].astype('str')\n",
    "\n",
    "df_train = df_train[['img_file', 'class']]\n",
    "df_test = df_test[['img_file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_split(params, df_train):\n",
    "    # training, validation set을\n",
    "    # 8:2비율로 Random한 index들로 split합니다. \n",
    "    \n",
    "    its = np.arange(df_train.shape[0])\n",
    "    train_idx, val_idx = train_test_split(its, train_size=0.8, random_state=params['random_state'])\n",
    "\n",
    "    X_train = df_train.iloc[train_idx, :]\n",
    "    X_val = df_train.iloc[val_idx, :]\n",
    "    \n",
    "    # 다음 번 split을 위해 random state 값을 변경합니다.\n",
    "    # random state 값을 지정한 이유는, 지정한 random state를 통해 index를 매번 갖게 split할 수 있어\n",
    "    # 디버깅에 용이하기 때문입니다.\n",
    "    \n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "def train_val_split_fixed(params, df_train, i, n):\n",
    "    # training, validation set을\n",
    "    # 8:2비율로 고정된 index들로 split합니다.\n",
    "    # 50 epochs 기준으로\n",
    "    # 모든 데이터를 반드시 4번씩 training 하게 되고,\n",
    "    # 모든 데이터는 반드시 1번씩 validaton 하게 됩니다.\n",
    "    \n",
    "    its = list(range(df_train.shape[0]))\n",
    "    chunks = [its[j::n] for j in range(n)]\n",
    "    \n",
    "    i %= n\n",
    "    \n",
    "    X_val = df_train.iloc[chunks[i], :]\n",
    "    chunks.pop(i)\n",
    "    X_train = df_train.iloc[sum(chunks, []), :]\n",
    "    \n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def make_generator(params, X_train, X_val):\n",
    "    # Define Generator config\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=params['rotation_range'],\n",
    "        width_shift_range=params['width_shift_range'],\n",
    "        height_shift_range=params['height_shift_range'],\n",
    "        shear_range=params['shear_range'],\n",
    "        zoom_range=params['zoom_range'],\n",
    "        horizontal_flip=params['horizontal_flip'],\n",
    "        brightness_range=params['brightness_range'],\n",
    "        preprocessing_function=preprocess_input)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "    # Make Generator\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=X_train, \n",
    "        directory=TRAIN_IMG_PATH,\n",
    "        x_col='img_file',\n",
    "        y_col='class',\n",
    "        target_size=params['img_size'],\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=params['batch_size'],\n",
    "        seed=params['random_state']\n",
    "    )\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=X_val, \n",
    "        directory=TRAIN_IMG_PATH,\n",
    "        x_col='img_file',\n",
    "        y_col='class',\n",
    "        target_size=params['img_size'],\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps(num_samples, batch_size):\n",
    "    if (num_samples % batch_size) > 0 :\n",
    "        return (num_samples // batch_size) + 1\n",
    "    else:\n",
    "        return num_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mybirth0407\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Model)             (None, 10, 10, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 196)               401604    \n",
      "=================================================================\n",
      "Total params: 21,263,084\n",
      "Trainable params: 21,208,556\n",
      "Non-trainable params: 54,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "cnn_model = Xception(include_top=False, input_shape=params['input_shape'])\n",
    "model = Sequential()\n",
    "model.add(cnn_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(196, activation='softmax', kernel_initializer='he_normal'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mybirth0407\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "{'random_state': 52, 'rotation_range': 10, 'width_shift_range': 0.2, 'height_shift_range': 0.2, 'shear_range': 0.5, 'zoom_range': 0.2, 'horizontal_flip': True, 'brightness_range': (0.7, 1.5), 'img_size': (299, 299), 'input_shape': (299, 299, 3), 'batch_size': 16, 'epochs_per_generator': 5, 'nb_workers': 6}\n",
      "Found 8012 images belonging to 196 classes.\n",
      "Found 2004 images belonging to 196 classes.\n",
      "Epoch 26/30\n",
      "160/501 [========>.....................] - ETA: 3:37 - loss: 0.4025 - acc: 0.8719"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# checkpoint에서 모델을 저장할 path\n",
    "filepath = os.path.join(DATA_PATH, 'xception_ep{epoch:03d}_vloss-{val_loss:.4f}_vacc-{val_acc:.4f}.h5')\n",
    "\n",
    "# 학습을 이어서 할 경우, model filename만 지정해주면 됩니다.\n",
    "from keras.models import load_model\n",
    "model_filename = 'xception_ep025_vloss-0.5598_vacc-0.8463.h5'\n",
    "model = load_model(os.path.join(DATA_PATH, model_filename))\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# 최대 학습 횟수\n",
    "epochs = 50\n",
    "\n",
    "for i in range(5, epochs // params['epochs_per_generator']):\n",
    "    print(params)\n",
    "    \n",
    "    X_train, X_val = train_val_split(params, df_train)\n",
    "    train_generator, validation_generator = make_generator(params, X_train, X_val)\n",
    "    \n",
    "    params.update({\n",
    "        'nb_train_samples': len(X_train),\n",
    "        'nb_validation_samples': len(X_val)\n",
    "    })\n",
    "    \n",
    "    histories.append(\n",
    "        model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=get_steps(params['nb_train_samples'], params['batch_size']),\n",
    "            # 한번 generate 된 데이터를 학습할 횟수\n",
    "            epochs=params['epochs_per_generator'] * (i + 1),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=get_steps(params['nb_validation_samples'], params['batch_size']),\n",
    "            callbacks=callbacks,\n",
    "            workers=params['nb_workers'],\n",
    "            initial_epoch=params['epochs_per_generator'] * i\n",
    "        )\n",
    "    )\n",
    "\n",
    "save_model_filename = 'xception_ep' + '{0:03d}'.format(epochs) + '_vloss-' +\\\n",
    "    str(round(histories[-1].history['val_loss'][-1], 4)) + '_vacc-' + str(round(histories[-1].history['val_acc'][-1], 4)) + '.h5'\n",
    "model.save(os.path.join(DATA_PATH, save_model_filename))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6169 images.\n",
      "386/386 [==============================] - 55s 143ms/step\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.models import load_model\n",
    "\n",
    "params.update({\n",
    "    'nb_test_samples': len(df_test)\n",
    "})\n",
    "\n",
    "model_filename = 'xception_ep021_vloss-0.4973_vacc-0.8568.h5'\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=TEST_IMG_PATH,\n",
    "    x_col='img_file',\n",
    "    y_col=None,\n",
    "    target_size=params['img_size'],\n",
    "    color_mode='rgb',\n",
    "    class_mode=None,\n",
    "    batch_size=params['batch_size'],\n",
    "    shuffle=False)\n",
    "\n",
    "model = load_model(os.path.join(DATA_PATH, model_filename))\n",
    "\n",
    "prediction = model.predict_generator(\n",
    "    generator = test_generator,\n",
    "    steps = get_steps(params['nb_test_samples'], params['batch_size']),\n",
    "    verbose=1,\n",
    "    workers=params['nb_workers']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 196 classes.\n",
      "Found 2004 images belonging to 196 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_file</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_00001.jpg</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_00002.jpg</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_00003.jpg</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_00004.jpg</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_00005.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         img_file class\n",
       "0  test_00001.jpg   124\n",
       "1  test_00002.jpg    98\n",
       "2  test_00003.jpg   157\n",
       "3  test_00004.jpg    94\n",
       "4  test_00005.jpg    16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "predicted_class_indices = np.argmax(prediction, axis=1)\n",
    "\n",
    "X_train, X_val = train_val_split(params, df_train)\n",
    "train_generator, _ = make_generator(params, X_train, X_val)\n",
    "\n",
    "# Generator class dictionary mapping\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v, k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n",
    "submission['class'] = predictions\n",
    "submission.to_csv(os.path.join(DATA_PATH, os.path.splitext(model_filename)[0] + '.csv'), index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reference:**\n",
    "https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c  \n",
    "https://keras.io/  \n",
    "http://www.arxiv.org/abs/1512.03385  \n",
    "https://pillow.readthedocs.io/en/stable/  \n",
    "https://www.kaggle.com/guglielmocamporese/macro-f1-score-keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
